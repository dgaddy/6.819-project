\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{van2009haarlet}
\citation{ong2004boosted}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Previous work}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Hand detection in images}{1}{subsection.2.1}}
\citation{van2011combining}
\citation{erol2007vision}
\citation{freeman1995orientation}
\citation{tompson2014real}
\citation{lee2011vision}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Hand gesture recognition}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Finger detection and gesture recognition}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Methods}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Problem setting}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Data collection}{2}{subsection.3.2}}
\citation{krizhevsky2012imagenet}
\citation{donahue2013decaf}
\citation{jia2014caffe}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012presentation}
\citation{krizhevsky2012presentation}
\citation{dalal2005histograms}
\citation{freeman1995orientation}
\citation{dalal2005histograms}
\citation{scikit-learn}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Overview of methods}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Convolutional neural networks}{3}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}Histogram of Gradients}{3}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}\hskip -1em.\nobreakspace  {}Regression}{3}{subsection.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}\hskip -1em.\nobreakspace  {}Smoothing}{3}{subsection.3.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}\hskip -1em.\nobreakspace  {}Evaluation}{3}{subsection.3.8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Results}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Performance on test data}{3}{subsection.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax }}{4}{table.caption.2}}
\newlabel{table:meanErrors}{{1}{4}{\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces This shows a neural network architecture like the one we used. We used the values of the layer indicated with an arrow as our features. Figure source: \cite  {krizhevsky2012presentation}\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cnn}{{1}{4}{This shows a neural network architecture like the one we used. We used the values of the layer indicated with an arrow as our features. Figure source: \cite {krizhevsky2012presentation}\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces This histogram shows the frequency of different sized errors by different methods. Errors are in pixels.\relax }}{5}{figure.caption.3}}
\newlabel{fig:hist}{{2}{5}{This histogram shows the frequency of different sized errors by different methods. Errors are in pixels.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces This shows the errors made on a set of test images using HOG features and kNN regression. Each arrow represents the error on a single image and points from the desired position to the position predicted by the system.\relax }}{5}{figure.caption.4}}
\newlabel{fig:hist}{{3}{5}{This shows the errors made on a set of test images using HOG features and kNN regression. Each arrow represents the error on a single image and points from the desired position to the position predicted by the system.\relax }{figure.caption.4}{}}
\newlabel{fig:pointImage}{{4a}{5}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:pointImage}{{a}{5}{\relax }{figure.caption.5}{}}
\newlabel{fig:pointLeft2Image}{{4b}{5}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:pointLeft2Image}{{b}{5}{\relax }{figure.caption.5}{}}
\newlabel{fig:pointHogEdit}{{4c}{5}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:pointHogEdit}{{c}{5}{\relax }{figure.caption.5}{}}
\newlabel{fig:pointLeft2HogEdit}{{4d}{5}{\relax }{figure.caption.5}{}}
\newlabel{sub@fig:pointLeft2HogEdit}{{d}{5}{\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The above images \ref  {fig:pointImage} and \ref  {fig:pointLeft2Image} demonstrate the correct output of our algorithm. The corresponding HOG features of each image can be seen below in \ref  {fig:pointHogEdit} and \ref  {fig:pointLeft2HogEdit}. The algorithm works well when the entire pointer finger is visible against the background as illustrated by \ref  {fig:pointImage} or when some portion of the pointer finger is visible against the background, even if much of it is located in front of the rest of the hand as shown in \ref  {fig:pointLeft2Image}.\relax }}{5}{figure.caption.5}}
\newlabel{fig:pointImages}{{4}{5}{The above images \ref {fig:pointImage} and \ref {fig:pointLeft2Image} demonstrate the correct output of our algorithm. The corresponding HOG features of each image can be seen below in \ref {fig:pointHogEdit} and \ref {fig:pointLeft2HogEdit}. The algorithm works well when the entire pointer finger is visible against the background as illustrated by \ref {fig:pointImage} or when some portion of the pointer finger is visible against the background, even if much of it is located in front of the rest of the hand as shown in \ref {fig:pointLeft2Image}.\relax }{figure.caption.5}{}}
\newlabel{fig:pointLeftImage}{{5a}{6}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:pointLeftImage}{{a}{6}{\relax }{figure.caption.6}{}}
\newlabel{fig:pointRightImage}{{5b}{6}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:pointRightImage}{{b}{6}{\relax }{figure.caption.6}{}}
\newlabel{fig:pointLeftHog}{{5c}{6}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:pointLeftHog}{{c}{6}{\relax }{figure.caption.6}{}}
\newlabel{fig:pointRightHog}{{5d}{6}{\relax }{figure.caption.6}{}}
\newlabel{sub@fig:pointRightHog}{{d}{6}{\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Examples of images in which our algorithm does not perform as well. The hand is pointing to different points on the screen in images \ref  {fig:pointLeftImage} and \ref  {fig:pointRightImage}. However, the HOG outputs in \ref  {fig:pointLeftHog} and \ref  {fig:pointRightHog} are almost identical. This happens because the HOG algorithm mostly detects the outline of the hand and not the pointing finger which is facing mostly toward the camera. Because the HOG images are so similar, the regression does not train properly, causing the blue cursor in \ref  {fig:pointLeftImage} and \ref  {fig:pointRightImage} to be in the incorrect location.\relax }}{6}{figure.caption.6}}
\newlabel{fig:pointErrors}{{5}{6}{Examples of images in which our algorithm does not perform as well. The hand is pointing to different points on the screen in images \ref {fig:pointLeftImage} and \ref {fig:pointRightImage}. However, the HOG outputs in \ref {fig:pointLeftHog} and \ref {fig:pointRightHog} are almost identical. This happens because the HOG algorithm mostly detects the outline of the hand and not the pointing finger which is facing mostly toward the camera. Because the HOG images are so similar, the regression does not train properly, causing the blue cursor in \ref {fig:pointLeftImage} and \ref {fig:pointRightImage} to be in the incorrect location.\relax }{figure.caption.6}{}}
\newlabel{fig:pointFarImage}{{6a}{6}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pointFarImage}{{a}{6}{\relax }{figure.caption.7}{}}
\newlabel{fig:pointCloseImage}{{6b}{6}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pointCloseImage}{{b}{6}{\relax }{figure.caption.7}{}}
\newlabel{fig:pointFarHog}{{6c}{6}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pointFarHog}{{c}{6}{\relax }{figure.caption.7}{}}
\newlabel{fig:pointCloseHog}{{6d}{6}{\relax }{figure.caption.7}{}}
\newlabel{sub@fig:pointCloseHog}{{d}{6}{\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The above images \ref  {fig:pointFarImage} and \ref  {fig:pointCloseImage} illustrate the difficulty in tracking the exact position of the pointer finger when it is pointing perpendicular to the camera. The finger in \ref  {fig:pointCloseImage} is pointing more toward the camera than the finger in \ref  {fig:pointFarImage}, but it is difficult for even a human to distinguish between the two. The HOG images \ref  {fig:pointFarHog} and \ref  {fig:pointCloseHog} are nearly indistinguishable as a result.\relax }}{6}{figure.caption.7}}
\newlabel{fig:close}{{6}{6}{The above images \ref {fig:pointFarImage} and \ref {fig:pointCloseImage} illustrate the difficulty in tracking the exact position of the pointer finger when it is pointing perpendicular to the camera. The finger in \ref {fig:pointCloseImage} is pointing more toward the camera than the finger in \ref {fig:pointFarImage}, but it is difficult for even a human to distinguish between the two. The HOG images \ref {fig:pointFarHog} and \ref {fig:pointCloseHog} are nearly indistinguishable as a result.\relax }{figure.caption.7}{}}
\bibstyle{ieee}
\bibdata{paper}
\bibcite{dalal2005histograms}{1}
\bibcite{donahue2013decaf}{2}
\bibcite{erol2007vision}{3}
\bibcite{freeman1995orientation}{4}
\bibcite{jia2014caffe}{5}
\bibcite{krizhevsky2012imagenet}{6}
\bibcite{krizhevsky2012presentation}{7}
\bibcite{lee2011vision}{8}
\bibcite{ong2004boosted}{9}
\bibcite{scikit-learn}{10}
\bibcite{tompson2014real}{11}
\bibcite{van2009haarlet}{12}
\bibcite{van2011combining}{13}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Evaluation as a computer interaction device}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion}{7}{section.5}}
