\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{dblfloatfix}
\usepackage{fixltx2e}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Detecting Hand Direction for Computer Interaction}

\author{Ben Mattinson and David Gaddy\\
Massachusetts Institute of Technology\\
77 Massachusetts Ave. Cambridge, MA 02139\\
{\tt\small bmatt@mit.edu, dgaddy@mit.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
We present a method for detecting the direction of a pointing hand in an image for the purpose of creating a computer interface based on pointing.  Given an image that is dominated by a pointing hand, we extract features and use regression to determine where it points.  We collected a set of images to use for comparing performance on this task.  We evaluated the performance of two different features for the regression: convolutional neural networks and histograms of oriented gradients.  We also compared two different methods of performing the regression: k-Nearest Neighbors and least squares linear regression.  This approach differs from many previous works in that it focuses on detecting the angle of a single finger in relation to the screen rather than on estimating hand poses or recognizing gestures.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Gesture recognition is an important problem for Human Computer Interaction because hands provide a very natural way to interact with the computer. Users are used to using their hands to manipulate the world and often use their hands when communicating ideas to other people. Giving computers the ability to recognize gestures would allow people to manipulate objects on a computer in a way that mimics the way they manipulate the world and communicate with others, instead of forcing them to use an artificial form of manipulation like a computer mouse. There are also other related applications for this technology, such as the interpretation of sign language.

The task of gesture recognition is often divided into two parts: first detecting hands in an image, and second recognizing the gesture a hand is making. Detecting the hands deals with finding local patches of pixels where a hand appears in an image. In order to do this, the computer must be able to differentiate hands from other objects in the background and from other parts of the body. Recognizing hands differs from many object recognition tasks, such as face detection, because the hands have many degrees of freedom that allow them to appear very different in images based on the gesture being made. For this reason, hand detection is often done by modeling skin color, but it has also been done by classifying shapes and combinations of the two methods.

Hand gesture recognition deals with the problem of, given a patch of pixels that represents a hand, determining the configuration the hand is in. One approach to this problem involves modeling the joints of the hand and using image data to determine the position of each joint, but because of the complexity of this model, an example based approach is often used instead. In the example based approach, hand images are compared against examples using various features to find an example with a similar hand configuration. This approach is similar to the problem of object classification since different configurations of a hand look like different objects when projected onto a two dimensional image.

%-------------------------------------------------------------------------
\section{Previous work}

\subsection{Hand detection in images}

Because the shape of hands varies greatly based on the position and configuration of a hand, color is often used as a primary feature for detecting hands in images. In \cite{van2009haarlet}, Bergh et al. modeled skin colors with a Gaussian Mixture Model of skin color and combined this with a color histogram of faces found with a face detector. By combining both the general skin color model with a scene specific skin color, they were able to detect skin accurately. To distinguish hands from other skin, they took the largest connected regions of skin that were not the face (determined using the face detector). Their method was reported to be more robust to lighting and different users than more basic color models, though they still encountered problems when faces and hands overlapped in images.

Other methods for detecting hands involve looking at shape. Ong and Bowden \cite{ong2004boosted} trained a tree of boosted classifiers to find hands in images. To deal with the many different possible shapes of the hands, the first level of the tree proposed possible hand locations that were then put through classifiers lower in the tree that were trained to detect specific hand configurations. They reported a 99.8\% success rate on hand detection.

One last cue used for detecting hands is depth information. To deal with the problems of detecting hands with color alone (namely overlapping skin regions), Bergh and Gool used depth images taken with an infrared depth camera in addition to color in \cite{van2011combining}. By placing the restriction that the hands be held a certain threshold in front of the face (determined using a combination of a face detector and depth image) they were able to accurately detect hand regions, even when they overlapped with other skin.

\subsection{Hand gesture recognition}

One approach to recognizing hand gestures is to build a model of the joints of the hand and to try to determine the location of these points on an image. Erol et al. give a survey of such techniques in \cite{erol2007vision}. The full kinematic model of a hand has 27 degrees of freedom. To deal with all of these degrees of freedom, constraints of the ranges of motion for each joint and joint angle dependencies are considered. In addition, methods such as placing markers on the hands or tracking the movement of hands through time have been used to make detecting the joint locations possible.

Another approach involves comparing the hand image to a set of example images for different gestures. This approach builds a classifier that attempts to put a given image into a class that represents the type of gesture being made. To do this, a vector of features is extracted from the image and compared against the features on the given examples. A variety of features and classifier types have been used to do this.

One approach, used by Freeman and Roth \cite{freeman1995orientation}, used a histogram of gradient orientations in the image as a feature vector. Using gradients made these features invariant to lighting and using a histogram made it invariant to translation. By finding the training feature vector that is most similar to the vector for a given image, they were able to distinguish between 10 different hand gestures.

More recently, neural networks have been employed for accurate continuous hand pose estimation. Tompson et al. \cite{tompson2014real} used a deep convolutional neural network to identify the positions of key points on hands in RGB+Depth images. They trained the neural network to recognize certain hand features and used the results to infer the hand pose.

\subsection{Finger detection and gesture recognition}

Several different methods of detecting fingers in images exist. Most involve first detecting a hand in the image and using information about the hand position and components to identify fingers. Hands can be detected using some of the aforementioned techniques, e.g. color or shape. Lee and Lee \cite{lee2011vision} used color and information about consecutive motion between frames of video for hand detection. After isolating the hand from the image, they examined the curvature of the hand region for sharp curves to identify fingertips. To measure the direction of each fingertip, Lee and Lee selected two points along the edge of the finger, one on each side a set distance from the fingertip. They then measured the direction of the vector between the average of these two points and the fingertip.

\section{Methods}

\subsection{Problem setting} % may want to rename this

For this work, we decided to focus on the problem of finding the orientation of a hand given a region where the hand is known to be instead of detecting the hand in a larger image.  To do this, we made several assumptions about the images we processed. First, we assume that the images have a clear background.  Although this assumption will often be broken in the target application of computer interaction, additional preprocessing could be done in a complete system to segment out the background of an image.  Methods for hand detection such as those described in the previous work section that use hand color, depth, or other cues could be used for this purpose.

Second, we assume that the hand is roughly at a constant distance from the camera and at roughly the center of the image.  For the application of pointing at a computer screen, this assumption is fairly reasonable since a user's distance to the screen will usually vary by only a few feet at most and is likely to hold their hand directly in front of the screen to point at it.  To make the system more robust or for other applications, the location of the hand in an image could be detected using the methods described in previous work and our system could be passed only the region around a detected image.  Additionally, we could make our system more robust to distance and location by training on images that contained hands at different distances and locations.

\subsection{Data collection}

We collected a set of images to evaluate the relative effectiveness of different methods.  These images were collected from a standard webcam directly above a computer screen.  Each picture contained a pointing hand on a blank background.  The hand was approximately one foot away from the camera and screen.  To keep hand location consistent, the user was shown a video of their hand as they recorded and attempted to keep their hand on a crosshair overlayed at the center of the image.  A dot was displayed on the screen and the user pointed at the dot as it moved across the screen.  The dot moved along a seven by seven grid with spacing of 80 pixels and for each location, a picture was taken of the user pointing at that location from the webcam.  We repeated this grid collection procedure 15 times, collecting a total of 735 images.


\subsection{Overview of methods}

To perform the task of deciding where a hand is pointing, we first created feature vectors to describe the images, then we used regression to predict where a hand was pointing.  We evaluated two different methods of extracting these feature vectors: convolutional neural networks and histograms of gradient orientations, which are described in detail below.

\subsection{Convolutional neural networks}

One source of features we evaluated was convolutional neural networks.  Convolutional neural networks have been used to get state of the art results in object recognition tasks such as the ILSVRC-2012 competition \cite{krizhevsky2012imagenet}.  They are made up of many layers of convolutional filters which are learned from training images, with each layer learning a higher representation of the image.   It has been found that the values of hidden units in the intermediate layers of a network provide good features for a variety of computer vision tasks, even tasks that the network was not trained for \cite{donahue2013decaf}.  For our features, we used the Caffe deep learning framework\cite{jia2014caffe} running a pretrained model architecture that is a variant of Krizhevsky's ILSVRC network\cite{krizhevsky2012imagenet}\footnote{The pretrained model is downloadable from the Caffe Model Zoo under the name 'BVLC Reference CaffeNet'}.  We ran their model and extracted the outputs of the final fully connected layer before the softmax layer, as shown in figure \ref{fig:cnn}, which gave us a 4096 dimensional feature vector.

\begin{figure*}[b]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\includegraphics[height=140px]{figures/cnn.png}
	\end{center}
	\caption{This shows a neural network architecture like the one we used.  We used the values of the layer indicated with an arrow as our features.  Figure source: \cite{krizhevsky2012presentation}}
	\label{fig:cnn}
\end{figure*}

\subsection{Histogram of Gradients}

We also evaluated the Histogram of Oriented Gradients (HOG) feature descriptor \cite{dalal2005histograms} as a means to detect the location and orientation of the pointing hand. This expands on work done by Freeman and Roth, who used gradients to detect hand gestures\cite{freeman1995orientation}. We used a variant of the R-HOG algorithm described by Dalal and Triggs in \cite{dalal2005histograms}. We divided the image into an 8 $\times$ 12 grid, yielding approximately 53 $\times$ 60 pixel cells. We divided orientations into 9 bins from $0\degree$-$360\degree$. In contrast to the block-normalization scheme used by Dalal and Triggs, all cells were scaled by the same value so the $l^2$ norm of the cells was 1. 
We did this rather than use a block normalization scheme to avoid amplifying noise in the blank background. The hand is the primary object in the frame and we want to preserve this after computing HOG.

\subsection{Regression}
We experimented with several different regression models including k-Nearest Neighbors and least squares linear regression.  We trained the k-Nearest Neighbors and linear regression models using the scikit-learn library\cite{scikit-learn}.  The models operated on the features vectors from HOG and the neural networks and predicted a location. The location predicted by k-Nearest Neighbors was ultimately used, and fed into a smoothing algorithm to compute the final location.

\subsection{Smoothing}
In order to make the system more robust to noise, we both eliminate outliers and smooth the location. Let $x[n]$ be the location at frame $n$ and $u[n]$ be the location output from the k-NN and linear regression combination at frame $n$. Outliers are eliminated if
\begin{equation}
	\sqrt{\sum_{i=n-l}^{n}(u[n]-x[i])^2} \geq \epsilon
\end{equation}
where $l$ and $\epsilon$ are parameters for tuning the amount of history to remember for computing outliers and the sensitivity to outliers, respectively. We used $l=10$ and $\epsilon=200$. Smoothing is implemented by the formula 
\begin{equation}
	x[n+1] = \left\{
		\begin{array}{l l}
			\lambda x[n] + (1-\lambda)u[n] & \qquad \text{if not an outlier} \\
			x[n] & \qquad \text{otherwise}
		\end{array}
	\right.
\end{equation}
 where $\lambda$ is a smoothing parameter between 0 and 1. We used $\lambda=0.5$ to balance quick responsiveness with robustness to noise.
 
\subsection{Evaluation}
For evaluation, we performed 15-fold cross validation, except the groups were chosen based on the grid collection iteration instead of a random sample.  For each data group, we trained on the other 14 groups and evaluated performance on the group, then we took the average performance for all 15 groups.  The reason for choosing groups based on collection iteration instead of random sampling is that images collected during the same scanning process tended to be more similar so training with images in the same iteration as the test data artificially increased scores.

It is important to note that the data collected has inherent noise because people are limited in precision for pointing at a location on a screen.  The location a user thinks they are pointing at may depend on the relative orientations of their eyes, their hands, and the screen.  For this reason, no system will be able to achieve perfect results on our data.

\section{Results}

\subsection{Performance on test data}
We evaluated the performance of both convolutional neural networks and HOG with k-Nearest Neighbors and Linear Regression. The results from our tests our given both in Table \ref{table:meanErrors}, which gives the average error, and in figure \ref{fig:hist}, which is a histogram of the errors of the different classifiers. The error is the euclidean distance measured in pixels between the predicted point and the actual point.

\begin{table}
	\caption{}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\textbf{Errors for Convolution Neural Nets}}\\\hline
			\textbf{Regression Type} & \textbf{Average Error (Pixels)} \\\hline
			k-Nearest Neighbors & 47.8 \\\hline
			Linear Regression &  114.2 \\\hline	
		\end{tabular}
		\begin{tabular}{|c|c|}
			\hline
			\multicolumn{2}{|c|}{\textbf{Errors for Histogram of Oriented Gradients}} \\\hline
			\textbf{Regression Type} & \textbf{Average Error (Pixels)} \\\hline
			k-Nearest Neighbors & 32.0\\\hline
			Linear Regression & 79.7 \\\hline
			Combined & 47.0 \\\hline
		\end{tabular}
	\end{center}
	\label{table:meanErrors}
\end{table}

The k-Nearest Neighbors algorithm when applied to HOG yields the highest accuracy as can be seen from both the mean error in Table \ref{table:meanErrors} and the error distribution in Figure \ref{fig:hist}. 
 
\begin{figure*}
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\includegraphics[width=0.9\linewidth]{figures/ClassifierErrorHist.png}
	\end{center}
	\caption{This histogram shows the frequency of different sized errors by different methods.  Errors are in pixels.}
	\label{fig:hist}
\end{figure*}

\begin{figure}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\includegraphics[width=0.9\linewidth]{figures/error_arrows.png}
	\caption{This shows the errors made on a set of test images using HOG features and kNN regression.  Each arrow represents the error on a single image and points from the desired position to the position predicted by the system.}
	\label{fig:hist}
\end{figure}

\begin{figure}[t]
	\begin{center}
		%\fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
		\begin{subfigure}[b]{0.45\linewidth}
			\includegraphics[width=\linewidth]{figures/PointImage.png}
			\caption{} % Just to get the (a)
			\label{fig:pointImage}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\linewidth}
			\includegraphics[width=\linewidth]{figures/PointLeft2Image.png}
			\caption{}
			\label{fig:pointLeft2Image}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\linewidth}
			\includegraphics[width=\linewidth]{figures/PointHOGedit.png}
			\caption{}
			\label{fig:pointHogEdit}
		\end{subfigure}
		\begin{subfigure}[b]{0.45\linewidth}
			\includegraphics[width=\linewidth]{figures/PointLeft2HOGedit.png}
			\caption{}
			\label{fig:pointLeft2HogEdit}
		\end{subfigure}
	\end{center}
	\caption{The above images \ref{fig:pointImage} and \ref{fig:pointLeft2Image} demonstrate the correct output of our algorithm. The corresponding HOG features of each image can be seen below in \ref{fig:pointHogEdit} and \ref{fig:pointLeft2HogEdit}. The algorithm works well when the entire pointer finger is visible against the background as illustrated by \ref{fig:pointImage} or when some portion of the pointer finger is visible against the background, even if much of it is located in front of the rest of the hand as shown in \ref{fig:pointLeft2Image}.}
	\label{fig:pointImages}
\end{figure}
\begin{center}
\begin{figure*}[t]
	% Figure on bad images
	\begin{center}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointLeftImage.png}
			\caption{} % Just to get the (a)
			\label{fig:pointLeftImage}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointRightImage.png}
			\caption{}
			\label{fig:pointRightImage}
		\end{subfigure}
		~\\
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointLeftHOG.png}
			\caption{}
			\label{fig:pointLeftHog}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointRightHOG.png}
			\caption{}
			\label{fig:pointRightHog}
		\end{subfigure}
	\end{center}
	\caption{Examples of images in which our algorithm does not perform as well. The hand is pointing to different points on the screen in images \ref{fig:pointLeftImage} and \ref{fig:pointRightImage}. However, the HOG outputs in \ref{fig:pointLeftHog} and \ref{fig:pointRightHog} are almost identical. This happens because the HOG algorithm mostly detects the outline of the hand and not the pointing finger which is facing mostly toward the camera. Because the HOG images are so similar, the regression does not train properly, causing the blue cursor in \ref{fig:pointLeftImage} and \ref{fig:pointRightImage} to be in the incorrect location.}
	\label{fig:pointErrors}
\end{figure*}
\begin{figure*}[b]
	% Figure on bad images
	\begin{center}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointFarImage.png}
			\caption{} % Just to get the (a)
			\label{fig:pointFarImage}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointCloseImage.png}
			\caption{}
			\label{fig:pointCloseImage}
		\end{subfigure}
		~\\
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointFarHOG.png}
			\caption{}
			\label{fig:pointFarHog}
		\end{subfigure}
		\begin{subfigure}[b]{0.3\textwidth}
			\includegraphics[width=\textwidth]{figures/PointCloseHOG.png}
			\caption{}
			\label{fig:pointCloseHog}
		\end{subfigure}
	\end{center}
	\caption{The above images \ref{fig:pointFarImage} and \ref{fig:pointCloseImage} illustrate the difficulty in tracking the exact position of the pointer finger when it is pointing perpendicular to the camera. The finger in \ref{fig:pointCloseImage} is pointing more toward the camera than the finger in \ref{fig:pointFarImage}, but it is difficult for even a human to distinguish between the two. The HOG images \ref{fig:pointFarHog} and \ref{fig:pointCloseHog} are nearly indistinguishable as a result.}
	\label{fig:close}
\end{figure*}
\end{center}
\FloatBarrier

\subsection{Evaluation as a computer interaction device}
	
To test our system's effectiveness for computer interaction, we used the system based on HOG features to predict where a user was pointing on a computer screen.  We used the prediction to move a dot on the screen, which simulated the movement of a computer mouse.

We found that the system had had some issues with hands pointing towards certain locations, such as directly towards the camera, or very close to the edges of the screen.  Figures \ref{fig:pointErrors} and \ref{fig:close} demonstrate these problems.  We also experienced problems with robustness to slight repositioning of the arm and hand.  We concluded that the system will need to improve before it could be used as a computer input device.

The time to process an image using convolutional neural networks was too high to allow for real time computer interaction, since each frame took several seconds to process.  Using a GPU to process frames would greatly increase the speed and should allow for the possibility of using these features in a real time system.

\section{Conclusion}
In this work, we explored detecting hand direction in images for interfacing with a computer.  We experimented with HOG as well as convolutional neural network features and tried several different regression methods.  To evaluate the effectiveness of different methods quantitatively, we tried our methods on a set of test images.  Using HOG features and a k-Nearest Neighbor regression, we achieved an average error of 32 pixels on these images.  Testing the system qualitatively, we found that the system had difficulty distinguishing between some hand directions and would need to be more robust to be used for computer interaction.

%-------------------------------------------------------------------------

{\small
\bibliographystyle{ieee}
\bibliography{paper}
}

\end{document}
